# â™Ÿï¸ Chess Reinforcement Learning Agents

## ğŸš€ Introduction

This project demonstrates the application of Reinforcement Learning (RL) techniques to train intelligent agents to play chess effectively. We implemented two state-of-the-art RL algorithmsâ€”**Deep Q-Network (DQN)** and **Proximal Policy Optimization (PPO)**â€”to develop agents capable of playing against each other and the renowned chess engine, Stockfish, at various skill levels (ELO ratings).

The agents operate within a custom Gymnasium environment integrated with **Python-chess** and **Stockfish**, learning to master chess through trial, error, and strategic reward shaping that evaluates material advantage, positional strength, and king safety.

## ğŸ“‚ Project Structure

```
chess_rl_project/
â”œâ”€â”€ environment/
â”‚   â”œâ”€â”€ custom_chess_env.py       # Custom Gymnasium Chess environment
â”‚   â”œâ”€â”€ rendering.py              # Chessboard visualization with OpenGL/PyGame
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ dqn_training.py           # DQN training script
â”‚   â”œâ”€â”€ ppo_training.py           # PPO training script
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ dqn/                      # Saved DQN models
â”‚   â”œâ”€â”€ ppo/                      # Saved PPO models
â”œâ”€â”€ tuning_results/
â”‚   â”œâ”€â”€ dqn_optuna_results/       # DQN hyperparameter tuning
â”‚   â””â”€â”€ ppo_optuna_results/       # PPO hyperparameter tuning
â”œâ”€â”€ videos/
â”‚   â”œâ”€â”€ dqn_vs_self.mp4           # DQN self-play
â”‚   â”œâ”€â”€ dqn_vs_stockfish.mp4      # DQN vs Stockfish
â”‚   â”œâ”€â”€ ppo_vs_self.mp4           # PPO self-play
â”‚   â””â”€â”€ ppo_vs_stockfish.mp4      # PPO vs Stockfish
â”œâ”€â”€ visualization/
â”‚   â”œâ”€â”€ screenshots/              # Chessboard visualization screenshots
â”œâ”€â”€ requirements.txt              # Project dependencies
â””â”€â”€ README.md                     # Project documentation
```

## âœ¨ Features

- âœ… Custom Gymnasium environment integrating with Python-chess and Stockfish
- âœ… State Space: 8x8x12 representation of chessboard
- âœ… Action Space: 4672 possible chess moves
- âœ… Two Reinforcement Learning algorithms implemented:
  - **Deep Q-Network (DQN)**
  - **Proximal Policy Optimization (PPO)**
- âœ… Hyperparameter optimization with **Optuna**
- âœ… Visualization using **OpenGL/PyGame**
- âœ… Gameplay capability against Stockfish engine (various ELO ratings)
- âœ… Reward structure evaluating material advantage, position, and king safety

## ğŸ¬ Video Demonstrations

| Description                  | Video                                  |
|------------------------------|----------------------------------------|
| DQN Agent vs Self            | [Watch Video](videos/dqn_vs_self.mp4) |
| DQN Agent vs Stockfish       | [Watch Video](videos/dqn_vs_stockfish.mp4) |
| PPO Agent vs Self            | [Watch Video](videos/ppo_vs_self.mp4) |
| PPO Agent vs Stockfish       | [Watch Video](videos/ppo_vs_stockfish.mp4) |

These videos illustrate gameplay, showcasing agent decisions, chessboard states, and real-time metrics.

## ğŸ¨ Visualizations

Chessboard visualization is implemented with OpenGL/PyGame, providing clear visual feedback of game states during agent training and evaluation. Screenshots below are from recorded gameplay videos:

![Chess Visualization](visualization/screenshots/chess_visual_1.png)
![Chess Visualization](visualization/screenshots/chess_visual_2.png)

## ğŸ› ï¸ Installation

Clone the repository and install dependencies:

```bash
git clone https://github.com/yourusername/chess_rl_project.git
cd chess_rl_project
pip install -r requirements.txt
```

## âš™ï¸ Usage

### Training Agents

```bash
# Train DQN agent
python training/dqn_training.py

# Train PPO agent
python training/ppo_training.py
```

### Hyperparameter Optimization

```bash
# Optimize DQN parameters
python tuning_results/dqn_optuna.py

# Optimize PPO parameters
python tuning_results/ppo_optuna.py
```

### Playing and Recording Games

```bash
# Record DQN agent playing Stockfish
python main.py --agent dqn --opponent stockfish --record

# Record PPO agent playing Stockfish
python main.py --agent ppo --opponent stockfish --record
```

## ğŸ“Š Results and Analysis

### Hyperparameter Optimization

| Algorithm | Optimal Hyperparameters                                | Best Reward |
|-----------|--------------------------------------------------------|-------------|
| **DQN**   | lr=0.00020578, gamma=0.9517, batch_size=32, exploration_fraction=0.2818, exploration_final_eps=0.2180, buffer_size=100000 | 28.45       |
| **PPO**   | lr=0.00092405, gamma=0.9519, batch_size=128, n_steps=1024, ent_coef=0.01824, gae_lambda=0.9415, clip_range=0.1387 | **36.60**   |

### Comparative Analysis
- **PPO** outperformed **DQN**, achieving higher and more stable rewards.
- PPO demonstrated faster convergence and greater generalization against Stockfish.
- DQN, while effective, showed greater variability and slower learning.

## ğŸ¤ Acknowledgments
- [Stockfish Chess Engine](https://stockfishchess.org)
- [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3)
- [Python-chess](https://github.com/niklasf/python-chess)
- [Optuna](https://github.com/optuna/optuna)

## ğŸ“„ License

This project is licensed under the **MIT License**.

